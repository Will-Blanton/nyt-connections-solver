{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc8b95f1aa52b31",
   "metadata": {},
   "source": [
    "Author: Will Blanton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3a617b097893e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:28:43.978879Z",
     "start_time": "2025-05-08T22:28:43.975485Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedc33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchltr.loss import LambdaNDCGLoss2\n",
    "from pytorchltr.evaluation import ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee765ae",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f82e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1820"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e51090bd299a90",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaae85caa03752d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:28:44.016078Z",
     "start_time": "2025-05-08T22:28:44.012884Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(word):\n",
    "    return pd.DataFrame({word:{\n",
    "        'length': len(word),\n",
    "        'num_vowels': sum(c in 'aeiou' for c in word.lower()),\n",
    "        'num_consonants': sum(c.isalpha() and c not in 'aeiou' for c in word.lower()),\n",
    "        'ends_with_ing': int(word.lower().endswith('ing')),\n",
    "        'ends_with_ed': int(word.lower().endswith('ed')),\n",
    "        'is_palindrome': int(word.lower() == word[::-1].lower())\n",
    "    }})\n",
    "\n",
    "def extract_structural_features(words):\n",
    "    \"\"\"\n",
    "    Extract basic structural features for the given words.\n",
    "    \"\"\"\n",
    "    return torch.tensor(pd.concat([extract_features(w) for w in words], axis=1).T.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d5d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_groups_and_labels():\n",
    "    \"\"\"\n",
    "    Produce index groups with graded relevance scores for each combination of words.\n",
    "\n",
    "    They're the same across all splits as long as word order is maintained. Model is order invariant, so this should be acceptable.\n",
    "    \"\"\"\n",
    "\n",
    "    # consecutive groups of 4 are groups (0-3, 4-7, ...)\n",
    "    groups = {\n",
    "        \"yellow\":{0, 1, 2, 3}, \n",
    "        \"green\": {4, 5, 6, 7},\n",
    "        \"blue\": {8, 9, 10, 11},\n",
    "        \"purple\": {12, 13, 14, 15}\n",
    "    }\n",
    "\n",
    "    quads = list(itertools.combinations(range(16), 4))\n",
    "\n",
    "    # produce graded relevance scores based on the max number of items in a group ([0,3] -> 4)\n",
    "    rel_scores = [max([len(set(q).intersection(g)) for _, g in groups.items()]) for q in quads]\n",
    "\n",
    "    return torch.tensor(quads), torch.tensor(rel_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3aba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "\n",
    "def collate_keep_wordlists(batch):\n",
    "    # batch is a list of dicts\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        vals = [b[k] for b in batch]\n",
    "        if k == \"word_list\":\n",
    "            # keep as list-of-lists grouped by sample: [ [w1..w16], ... ] (len = B)\n",
    "            out[k] = vals\n",
    "        else:\n",
    "            out[k] = default_collate(vals)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cb0e43f07f9f4",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558c49f1dd2cc80b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:28:44.033951Z",
     "start_time": "2025-05-08T22:28:44.029339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fff927f7b10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b95ae443a15a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:28:44.039522Z",
     "start_time": "2025-05-08T22:28:44.035954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d77bc8343505cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:28:44.063045Z",
     "start_time": "2025-05-08T22:28:44.060526Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: try prompt with sentence transformer\n",
    "word_embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5422c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = word_embedder.encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226e9588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = len(test_emb)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b34144383ae063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:29:20.496530Z",
     "start_time": "2025-05-08T22:28:44.083228Z"
    }
   },
   "outputs": [],
   "source": [
    "# ft = load_facebook_vectors('pretrained/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6285c481f2bc5948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:29:20.507675Z",
     "start_time": "2025-05-08T22:29:20.496530Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastTextEmbedder:\n",
    "    \"\"\"\n",
    "    Wrap gensim embeddings for simpler conversion to tensors and to fit with sentence-transformers api for easy swapping\n",
    "    \"\"\"\n",
    "    def __init__(self, ft_model):\n",
    "        self.ft = ft_model\n",
    "        self.dim = ft_model.vector_size\n",
    "\n",
    "    def encode(self, word_list, convert_to_tensor=True):\n",
    "        vectors = []\n",
    "        for word in word_list:\n",
    "            vectors.append(self.ft[word])\n",
    "\n",
    "        vectors = np.stack(vectors)\n",
    "\n",
    "        if convert_to_tensor:\n",
    "            return torch.tensor(vectors, dtype=torch.float)\n",
    "        else:\n",
    "            return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d853f2a1222e211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:29:20.514687Z",
     "start_time": "2025-05-08T22:29:20.509675Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_embedder = FastTextEmbedder(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ea9b0125b7dbb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:36:51.712972Z",
     "start_time": "2025-05-08T22:36:51.700734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>connections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>palindromes</td>\n",
       "      <td>[kayak, level, mom, race car]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>keyboard keys</td>\n",
       "      <td>[option, return, shift, tab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>nba teams</td>\n",
       "      <td>[bucks, heat, jazz, nets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>wet weather</td>\n",
       "      <td>[hail, rain, sleet, snow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>letter homophones</td>\n",
       "      <td>[are, queue, sea, why]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           category                    connections\n",
       "0  2023-06-12        palindromes  [kayak, level, mom, race car]\n",
       "1  2023-06-12      keyboard keys   [option, return, shift, tab]\n",
       "2  2023-06-12          nba teams      [bucks, heat, jazz, nets]\n",
       "3  2023-06-12        wet weather      [hail, rain, sleet, snow]\n",
       "4  2023-06-13  letter homophones         [are, queue, sea, why]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections_df = pd.read_csv('data/connections.csv', index_col=0)\n",
    "# connections_df.drop(columns='category', inplace=True)\n",
    "\n",
    "# fix issues with incorrect format... (not worth going in and adding a mechanism to correct...)\n",
    "connections_df.loc[1298, \"connections\"] = \"['line', 'plane', 'point', 'solid']\"\n",
    "connections_df.loc[1892, \"connections\"] = \"['abyss', 'fly', 'matrix', 'thing']\"\n",
    "\n",
    "# remove april fools samples since they include emojis or other potentially noisy samples\n",
    "connections_df = connections_df[~connections_df['date'].str.contains(\"04-01\")]\n",
    "connections_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "connections_df['connections'] = connections_df['connections'].apply(ast.literal_eval)\n",
    "connections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e69e0c25f90958a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:29:20.675625Z",
     "start_time": "2025-05-08T22:29:20.660566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kayak',\n",
       " 'level',\n",
       " 'mom',\n",
       " 'race car',\n",
       " 'option',\n",
       " 'return',\n",
       " 'shift',\n",
       " 'tab',\n",
       " 'bucks',\n",
       " 'heat']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = list(itertools.chain.from_iterable(\n",
    "    connections_df\n",
    "    .groupby('date')['connections']\n",
    "    .agg(lambda lists: list(itertools.chain.from_iterable(lists)))\n",
    "    .values\n",
    "))\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12d4dfaed1e3240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T22:29:20.680163Z",
     "start_time": "2025-05-08T22:29:20.675625Z"
    }
   },
   "outputs": [],
   "source": [
    "# [w for w in all_words if ('_' in w) or ('-' in w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d00d8a6d0468304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 3))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">input&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;content&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('encoding',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">encoding&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;utf-8&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decode_error',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decode_error&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;strict&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strip_accents',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strip_accents&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('lowercase',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">lowercase&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('preprocessor',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">preprocessor&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tokenizer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tokenizer&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('stop_words',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">stop_words&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('token_pattern',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">token_pattern&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;(?u)\\\\b\\\\w\\\\w+\\\\b&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ngram_range',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ngram_range&nbsp;</td>\n",
       "            <td class=\"value\">(1, ...)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('analyzer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">analyzer&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;char&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_df&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_df&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('vocabulary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">vocabulary&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('binary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">binary&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.int64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "vectorizer.fit(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b00bf121815d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:42:03.162112Z",
     "start_time": "2025-05-08T21:42:03.153907Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TODO: add more features to handle complicated cases\n",
    "- phonetic word embeddings\n",
    "- character-level embeddings (either trained via RNN or pre-trained)\n",
    "- n-gram?\n",
    "\"\"\"\n",
    "class ConnectionsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        puzzle_df: pd.DataFrame,\n",
    "        word_emb_model,\n",
    "        char_vectorizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.puzzle_df = puzzle_df\n",
    "        self.puzzle_list = []\n",
    "\n",
    "        # get lists of words for each puzzle \n",
    "        words_per_date = (\n",
    "            self.puzzle_df\n",
    "            .groupby('date')['connections']\n",
    "            .agg(lambda lists: list(itertools.chain.from_iterable(lists)))\n",
    "        )\n",
    "\n",
    "        # create data object for each puzzle\n",
    "        for date, word_list in words_per_date.items():\n",
    "            if len(word_list) != 16:\n",
    "                raise ValueError(f'Word list length {len(word_list)} does not match 16 words')\n",
    "\n",
    "            # create node features \n",
    "            x = word_emb_model.encode(word_list, convert_to_tensor=True).cpu()\n",
    "\n",
    "            # don't need labels, as all labels are the same...\n",
    "            puzzle = {\n",
    "                \"x\": x,\n",
    "                \"x_char\": torch.tensor(char_vectorizer.transform(word_list).toarray()),\n",
    "                # \"x_struct\": extract_structural_features(word_list),\n",
    "                \"word_list\": word_list\n",
    "            } \n",
    "\n",
    "            self.puzzle_list.append(puzzle)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.puzzle_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.puzzle_list[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855f81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, y = construct_groups_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "249eddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 0,  1,  2,  4],\n",
       "        [ 0,  1,  2,  5],\n",
       "        [ 0,  1,  2,  6],\n",
       "        [ 0,  1,  2,  7],\n",
       "        [ 0,  1,  2,  8],\n",
       "        [ 0,  1,  2,  9],\n",
       "        [ 0,  1,  2, 10],\n",
       "        [ 0,  1,  2, 11],\n",
       "        [ 0,  1,  2, 12]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3778440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d44c5222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78608e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b83dd9ed75d2b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:42:03.167527Z",
     "start_time": "2025-05-08T21:42:03.163116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 853\n",
      "Train length: 682\n",
      "Val length: 86\n",
      "Test length: 85\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "dates = connections_df[\"date\"].unique()\n",
    "train_idx = random.sample(range(len(dates)), k=int(len(dates) * .8))\n",
    "train_dates = dates[train_idx]\n",
    "test_dates = dates[~np.isin(range(len(dates)), train_idx)]\n",
    "\n",
    "test_dates = np.random.permutation(test_dates)\n",
    "split_idx = len(test_dates) // 2\n",
    "val_dates = test_dates[split_idx:]\n",
    "test_dates = test_dates[:split_idx]\n",
    "\n",
    "print(f\"Total Length: {len(dates)}\")\n",
    "print(f\"Train length: {len(train_dates)}\")\n",
    "print(f\"Val length: {len(val_dates)}\")\n",
    "print(f\"Test length: {len(test_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39c29eb28524c1cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:42:06.631481Z",
     "start_time": "2025-05-08T21:42:03.168531Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train = ConnectionsDataset(connections_df.loc[connections_df[\"date\"].isin(train_dates)], word_embedder, vectorizer)\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_keep_wordlists)\n",
    "\n",
    "val = ConnectionsDataset(connections_df.loc[connections_df[\"date\"].isin(val_dates)], word_embedder, vectorizer)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_keep_wordlists)\n",
    "\n",
    "test = ConnectionsDataset(connections_df.loc[connections_df[\"date\"].isin(test_dates)], word_embedder, vectorizer)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_keep_wordlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c6e2d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a97fca80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1820, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c32813e2d229c248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:42:07.468811Z",
     "start_time": "2025-05-08T21:42:07.007412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'radius', 'reverse', 'right', 'babe', 'fox', 'snack', 'ten', 'bear', 'generate', 'produce', 'yield', 'aside', 'detour', 'digression', 'tangent']\n",
      "x: torch.Size([2, 16, 384])\n",
      "x type: torch.float32\n",
      "x_char: torch.Size([2, 16, 4800])\n",
      "torch.Size([2, 1820, 4, 384])\n",
      "tensor([[-0.0079,  0.0449,  0.0086,  ..., -0.0353,  0.1046, -0.0347],\n",
      "        [ 0.0729,  0.0323, -0.1690,  ...,  0.0757,  0.0324,  0.0148],\n",
      "        [-0.0048,  0.0567,  0.0470,  ...,  0.0278,  0.0530, -0.0354],\n",
      "        [-0.0549,  0.0476, -0.0326,  ...,  0.0379,  0.0735,  0.0799]])\n"
     ]
    }
   ],
   "source": [
    "for batch in DataLoader(val, batch_size=2, shuffle=True, collate_fn=collate_keep_wordlists):\n",
    "    print(batch[\"word_list\"][0])\n",
    "\n",
    "    x = batch[\"x\"]\n",
    "    print(f\"x: {x.shape}\")\n",
    "    print(f\"x type: {x.dtype}\")\n",
    "\n",
    "    \n",
    "    x_char = batch[\"x_char\"]\n",
    "    print(f\"x_char: {x_char.shape}\")\n",
    "\n",
    "    idx_flat = idx.reshape(-1)  # (1820*4,)\n",
    "    group_embeds = torch.index_select(x, dim=1, index=idx_flat).view(x.shape[0], 1820, 4, x.shape[2])\n",
    "    print(group_embeds.shape)\n",
    "    print(group_embeds[0][0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a02065fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1820, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b1a0e6a350400",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51cebf",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2bb1f25ca556d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:08:05.260342Z",
     "start_time": "2025-05-08T21:08:05.254536Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiheadAttentionBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim,\n",
    "            out_dim,\n",
    "            attn_heads=8\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # following the notation from the set transformer paper\n",
    "        self.attn = torch.nn.MultiheadAttention(in_dim, num_heads=attn_heads, batch_first=True, dropout=.3)\n",
    "        self.h_norm = torch.nn.LayerNorm(in_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, out_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.mab_norm = torch.nn.LayerNorm(in_dim)\n",
    " \n",
    "    def forward(self, x, y):\n",
    "        # H: self-attention with residual\n",
    "        x = self.h_norm(x + self.attn(x, y, y)[0])\n",
    "\n",
    "        # MAB \n",
    "        return self.mab_norm(x + self.ff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aa9d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dims: List[int],\n",
    "        num_mabs: int,\n",
    "        attn_heads: List[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_mabs == len(out_dims) == len(attn_heads), \"num_mabs, out_dims, attn_heads must match in length\"\n",
    "\n",
    "        mabs = []\n",
    "        cur_in = in_dim\n",
    "        for i in range(num_mabs):\n",
    "            mabs.append(MultiheadAttentionBlock(\n",
    "                in_dim=cur_in,\n",
    "                out_dim=out_dims[i],\n",
    "                attn_heads=attn_heads[i],\n",
    "            ))\n",
    "            cur_in = out_dims[i]\n",
    "        self.mabs = nn.ModuleList(mabs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for mab in self.mabs:\n",
    "            x = mab(x, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d801444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim,\n",
    "            h1,\n",
    "            out_dim,\n",
    "            num_seeds,\n",
    "            h2 = None,\n",
    "            mab_attn_heads=8,\n",
    "            sab_attn_heads=8,\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                k: number of seed vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_seeds = num_seeds\n",
    "\n",
    "        # TODO: use Xavier intiializations?\n",
    "        self.seed_vectors = nn.Parameter(torch.randn(num_seeds, in_dim))        \n",
    "        \n",
    "        self.ff1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, h1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mab = MultiheadAttentionBlock(\n",
    "                in_dim=h1,\n",
    "                out_dim=h2 if num_seeds > 1 else out_dim,\n",
    "                attn_heads=mab_attn_heads,\n",
    "        )\n",
    "\n",
    "        # only need to use the set encoder when more than 1 vector is produced (model interactions between)\n",
    "        if num_seeds > 1:\n",
    "            self.sab = SetEncoder(h2, [h2], [sab_attn_heads])\n",
    "\n",
    "            self.ff2 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(h2, out_dim),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.ff1(x)\n",
    "        x = self.mab(self.source_vectors, x)\n",
    "\n",
    "        if self.num_seeds > 1:\n",
    "            x = self.sab(x)\n",
    "            x = self.ff2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac49db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPScorer(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dims=[256, 128], dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Generic configurable MLP scorer.\n",
    "\n",
    "        Args:\n",
    "            input_dim:  size of input feature vector (e.g., 4*D)\n",
    "            hidden_dims: list/tuple of hidden layer sizes\n",
    "            output_dim: size of output (1 for scalar score)\n",
    "            dropout: dropout probability between hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = in_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_dim)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cad061f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grouper(nn.Module):\n",
    "    def __init__(self, group_idx: torch.Tensor):\n",
    "        super().__init__()\n",
    "        # Ensure long dtype and register as a buffer so .to(device)/.cuda() moves it\n",
    "        self.num_groups, self.group_size = group_idx.shape\n",
    "        self.register_buffer(\"idx\", group_idx.to(dtype=torch.long), persistent=False)\n",
    "\n",
    "    def forward(self, x):  #\n",
    "        \"\"\"\n",
    "        Group items in x into all combinations of 4 items\n",
    "        \"\"\"\n",
    "        B, _, D = x.shape\n",
    "        idx_flat = self.idx.reshape(-1)                      # (1820*4,)\n",
    "        out = torch.index_select(x, dim=1, index=idx_flat)   # (B, 1820*4, D)\n",
    "        return out.view(B, self.num_groups, self.group_size, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee546e9",
   "metadata": {},
   "source": [
    "## Baseline (MLP) - No Global Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc0883bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a simple model and build our way up\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, in_dim, group_idx, layers=[512, 256, 256]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.grouper = Grouper(group_idx)\n",
    "        self.scorer = MLPScorer(256 * self.grouper.group_size, hidden_dims=layers, dropout=.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.grouper(x) # (B, 1820, 4, D)\n",
    "\n",
    "        # concatenate for scoring\n",
    "        x = x.flatten(2, 3) # (B, 1820, D * 4)\n",
    "\n",
    "        return self.scorer(x)  # (B, 1820, 1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a2671",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "343623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start with a simple model and build our way up\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, in_dim, group_idx, layers=[512, 256, 256], attn_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.grouper = Grouper(group_idx)\n",
    "        self.encoder = SetEncoder(\n",
    "            256,\n",
    "            [256] * attn_layers,\n",
    "            attn_layers,\n",
    "            [8] * attn_layers\n",
    "        )\n",
    "        self.scorer = MLPScorer(256 * self.grouper.group_size, hidden_dims=layers, dropout=.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.grouper(x) # (B, 1820, 4, D)\n",
    "\n",
    "        # concatenate for scoring\n",
    "        x = x.flatten(2, 3) # (B, 1820, D * 4)\n",
    "\n",
    "        return self.scorer(x)  # (B, 1820, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab964a",
   "metadata": {},
   "source": [
    "## Set Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b99d5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class SetTransformer(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             in_dim,\n",
    "#         ):\n",
    "#         super().__init__() \n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    " \n",
    "#         # combine input features into a single representation\n",
    "\n",
    "#         # encoder layers\n",
    "\n",
    "#         # decode into cluster centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ae2fad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 3,  ..., 3, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ee2ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1820])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b9b98af60c988",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5ad830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_overlap(pred_scores, relevance, k=4):\n",
    "    \"\"\"\n",
    "    Computes fraction of relevant items appearing in the model's top-k predictions.\n",
    "    Args:\n",
    "        pred_scores (torch.Tensor): (B, N) predicted scores\n",
    "        relevance (torch.Tensor): (B, N) binary relevance labels (0 or 1)\n",
    "        k (int): number of top items to consider\n",
    "    Returns:\n",
    "        torch.Tensor: mean overlap fraction across batch\n",
    "    \"\"\"\n",
    "    _, pred_idx = torch.topk(pred_scores, k, dim=1)\n",
    "    _, true_idx = torch.topk(relevance, k, dim=1)\n",
    "\n",
    "    overlap = (pred_idx.unsqueeze(2) == true_idx.unsqueeze(1)).any(dim=2).float().sum(dim=1)\n",
    "    return (overlap / k).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14dd187951a36fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:08:05.332531Z",
     "start_time": "2025-05-08T21:08:05.328486Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn, n_scalar, device, k=4):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_ndcg = 0\n",
    "    total_top4_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            \n",
    "            B, _, D = x.shape\n",
    " \n",
    "            scores = model(x)\n",
    "\n",
    "            # expand to use in batch\n",
    "            relevance = y.unsqueeze(0).expand(B, len(y)) # (B, 1820)\n",
    "\n",
    "            n = n_scalar.expand(B)\n",
    "\n",
    "            total_loss += loss_fn(scores, relevance, n).mean().item()\n",
    "            total_ndcg += ndcg(scores, relevance, n, k=k).mean().item()\n",
    "            total_top4_acc = topk_overlap(scores.squeeze(-1), relevance, k=4).item()\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_ndcg = total_ndcg / len(loader)\n",
    "    avg_top4_acc = total_top4_acc / len(loader)\n",
    "\n",
    "    return avg_loss, avg_ndcg, avg_top4_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65d5e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "def train(model, y, num_epochs, model_name, device, lr=1e-4):\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{model_name}/connections_experiment_LTR-{current_time}\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    y = y.to(device)\n",
    "    n_scalar = torch.tensor(M, device=device, dtype=torch.long)\n",
    "\n",
    "    loss = LambdaNDCGLoss2().to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_ndcg = 0\n",
    "        train_top4_acc = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            x = batch[\"x\"].to(device)\n",
    "            \n",
    "            B, _, D = x.shape\n",
    "            \n",
    "            # expand to use in batch\n",
    "            relevance = y.unsqueeze(0).expand(B, len(y)) # (B, 1820)\n",
    "            n = n_scalar.expand(B)\n",
    "\n",
    "            with torch.amp.autocast(dtype=torch.float16, device_type=str(device)):\n",
    "                scores = model(x)\n",
    "                batch_loss = loss(scores, relevance, n).mean()\n",
    "\n",
    "            scaler.scale(batch_loss).backward()\n",
    "            # TODO: check if gradient clipping is needed\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # eval train\n",
    "            with torch.no_grad():\n",
    "                train_ndcg += ndcg(scores, relevance, n, k=4).mean().item()\n",
    "                train_top4_acc += topk_overlap(scores.squeeze(-1), relevance, k=4).item()\n",
    "                \n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_ndcg /= len(train_loader)\n",
    "        train_top4_acc /= len(train_loader)\n",
    "\n",
    "        val_loss, val_ndcg, val_top4_acc = evaluate(model, val_loader, loss, n_scalar, device)\n",
    "        test_loss, test_ndcg, test_top4_acc = evaluate(model, test_loader, loss, n_scalar, device)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            # --- Tensorboard logging ---\n",
    "            writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "            writer.add_scalar(\"loss/test\", test_loss, epoch)\n",
    "\n",
    "            writer.add_scalar(\"ndcg/train\", train_ndcg, epoch)\n",
    "            writer.add_scalar(\"ndcg/val\", val_ndcg, epoch)\n",
    "            writer.add_scalar(\"ndcg/test\", test_ndcg, epoch)\n",
    "\n",
    "            writer.add_scalar(\"top4_acc/train\", train_top4_acc, epoch)\n",
    "            writer.add_scalar(\"top4_acc/val\", val_top4_acc, epoch)\n",
    "            writer.add_scalar(\"top4_acc/test\", test_top4_acc, epoch)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"lr\", current_lr, epoch)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                writer.add_histogram(f\"param_hist/{name}\", param, epoch)\n",
    "\n",
    "                if param.grad is not None:\n",
    "                    writer.add_histogram(f\"grad_hist/{name}\", param.grad, epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Train NDCG: {train_ndcg:.4f} | \"\n",
    "                f\"Train Top-4 Acc: {train_top4_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val NDCG: {val_ndcg:.4f} | \"\n",
    "                f\"Val Top-4 Acc: {val_top4_acc:.4f} | \"\n",
    "                f\"Test Loss: {test_loss:.4f} | \"\n",
    "                f\"Test NDCG: {test_ndcg:.4f} | \"\n",
    "                f\"Test Top-4 Acc: {test_top4_acc:.4f}\"\n",
    "            )\n",
    "    \n",
    "    return model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23567be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 100\n",
    "M = 1820\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ce861",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ce4e8298d3c17ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:12:51.170265Z",
     "start_time": "2025-05-08T21:09:31.176414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 1.7046 | Train NDCG: 0.2236 | Train Top-4 Acc: 0.0040 | Val Loss: 1.6662 | Val NDCG: 0.2492 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6666 | Test NDCG: 0.2714 | Test Top-4 Acc: 0.0000\n",
      "Epoch 5 | Train Loss: 1.5732 | Train NDCG: 0.3479 | Train Top-4 Acc: 0.0247 | Val Loss: 1.6518 | Val NDCG: 0.2972 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6577 | Test NDCG: 0.2925 | Test Top-4 Acc: 0.0000\n",
      "Epoch 10 | Train Loss: 1.3414 | Train NDCG: 0.4478 | Train Top-4 Acc: 0.0563 | Val Loss: 1.6419 | Val NDCG: 0.3171 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6646 | Test NDCG: 0.2834 | Test Top-4 Acc: 0.0000\n",
      "Epoch 15 | Train Loss: 1.1265 | Train NDCG: 0.4860 | Train Top-4 Acc: 0.0674 | Val Loss: 1.6447 | Val NDCG: 0.3107 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6571 | Test NDCG: 0.2903 | Test Top-4 Acc: 0.0000\n",
      "Epoch 20 | Train Loss: 0.9425 | Train NDCG: 0.5072 | Train Top-4 Acc: 0.0802 | Val Loss: 1.6524 | Val NDCG: 0.3027 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6658 | Test NDCG: 0.2830 | Test Top-4 Acc: 0.0000\n",
      "Epoch 25 | Train Loss: 0.7817 | Train NDCG: 0.5236 | Train Top-4 Acc: 0.1073 | Val Loss: 1.6524 | Val NDCG: 0.2994 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6658 | Test NDCG: 0.2854 | Test Top-4 Acc: 0.0000\n",
      "Epoch 30 | Train Loss: 0.7662 | Train NDCG: 0.5202 | Train Top-4 Acc: 0.0983 | Val Loss: 1.6429 | Val NDCG: 0.3089 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6608 | Test NDCG: 0.2837 | Test Top-4 Acc: 0.0000\n",
      "Epoch 35 | Train Loss: 0.7507 | Train NDCG: 0.5216 | Train Top-4 Acc: 0.1028 | Val Loss: 1.6521 | Val NDCG: 0.3038 | Val Top-4 Acc: 0.0139 | Test Loss: 1.6650 | Test NDCG: 0.2875 | Test Top-4 Acc: 0.0000\n",
      "Epoch 40 | Train Loss: 0.7479 | Train NDCG: 0.5267 | Train Top-4 Acc: 0.1081 | Val Loss: 1.6440 | Val NDCG: 0.3071 | Val Top-4 Acc: 0.0139 | Test Loss: 1.6651 | Test NDCG: 0.2900 | Test Top-4 Acc: 0.0083\n",
      "Epoch 45 | Train Loss: 0.7494 | Train NDCG: 0.5251 | Train Top-4 Acc: 0.1061 | Val Loss: 1.6440 | Val NDCG: 0.3018 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6642 | Test NDCG: 0.2827 | Test Top-4 Acc: 0.0000\n",
      "Epoch 50 | Train Loss: 0.7485 | Train NDCG: 0.5212 | Train Top-4 Acc: 0.0986 | Val Loss: 1.6393 | Val NDCG: 0.3055 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6567 | Test NDCG: 0.2869 | Test Top-4 Acc: 0.0000\n",
      "Epoch 55 | Train Loss: 0.7462 | Train NDCG: 0.5229 | Train Top-4 Acc: 0.1033 | Val Loss: 1.6433 | Val NDCG: 0.3019 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6671 | Test NDCG: 0.2728 | Test Top-4 Acc: 0.0000\n",
      "Epoch 60 | Train Loss: 0.7493 | Train NDCG: 0.5265 | Train Top-4 Acc: 0.1130 | Val Loss: 1.6438 | Val NDCG: 0.3092 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6600 | Test NDCG: 0.2897 | Test Top-4 Acc: 0.0000\n",
      "Epoch 65 | Train Loss: 0.7480 | Train NDCG: 0.5255 | Train Top-4 Acc: 0.1040 | Val Loss: 1.6475 | Val NDCG: 0.2964 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6658 | Test NDCG: 0.2814 | Test Top-4 Acc: 0.0000\n",
      "Epoch 70 | Train Loss: 0.7478 | Train NDCG: 0.5206 | Train Top-4 Acc: 0.1006 | Val Loss: 1.6432 | Val NDCG: 0.2985 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6640 | Test NDCG: 0.2857 | Test Top-4 Acc: 0.0167\n",
      "Epoch 75 | Train Loss: 0.7479 | Train NDCG: 0.5239 | Train Top-4 Acc: 0.1066 | Val Loss: 1.6462 | Val NDCG: 0.3020 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6661 | Test NDCG: 0.2882 | Test Top-4 Acc: 0.0083\n",
      "Epoch 80 | Train Loss: 0.7471 | Train NDCG: 0.5220 | Train Top-4 Acc: 0.1013 | Val Loss: 1.6490 | Val NDCG: 0.2984 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6626 | Test NDCG: 0.2906 | Test Top-4 Acc: 0.0000\n",
      "Epoch 85 | Train Loss: 0.7486 | Train NDCG: 0.5301 | Train Top-4 Acc: 0.1116 | Val Loss: 1.6453 | Val NDCG: 0.3005 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6576 | Test NDCG: 0.2834 | Test Top-4 Acc: 0.0000\n",
      "Epoch 90 | Train Loss: 0.7476 | Train NDCG: 0.5270 | Train Top-4 Acc: 0.1101 | Val Loss: 1.6488 | Val NDCG: 0.2947 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6688 | Test NDCG: 0.2818 | Test Top-4 Acc: 0.0000\n",
      "Epoch 95 | Train Loss: 0.7480 | Train NDCG: 0.5245 | Train Top-4 Acc: 0.1049 | Val Loss: 1.6436 | Val NDCG: 0.3083 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6663 | Test NDCG: 0.2855 | Test Top-4 Acc: 0.0000\n",
      "Epoch 99 | Train Loss: 0.7501 | Train NDCG: 0.5217 | Train Top-4 Acc: 0.1028 | Val Loss: 1.6466 | Val NDCG: 0.3068 | Val Top-4 Acc: 0.0139 | Test Loss: 1.6629 | Test NDCG: 0.2824 | Test Top-4 Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = Baseline(\n",
    "    in_dim=D,\n",
    "    group_idx=idx,\n",
    "    layers=[1024, 512, 256]\n",
    ").to(device)\n",
    "\n",
    "_ = train(model, y, num_epochs=100, model_name=\"baseline\", device=device, lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385d218",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac9d7faec9a1040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 1.7061 | Train NDCG: 0.2185 | Train Top-4 Acc: 0.0025 | Val Loss: 1.6727 | Val NDCG: 0.2641 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6701 | Test NDCG: 0.2638 | Test Top-4 Acc: 0.0000\n",
      "Epoch 5 | Train Loss: 1.6053 | Train NDCG: 0.3204 | Train Top-4 Acc: 0.0185 | Val Loss: 1.6515 | Val NDCG: 0.2862 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6670 | Test NDCG: 0.2924 | Test Top-4 Acc: 0.0167\n",
      "Epoch 10 | Train Loss: 1.4271 | Train NDCG: 0.4082 | Train Top-4 Acc: 0.0338 | Val Loss: 1.6475 | Val NDCG: 0.3116 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6550 | Test NDCG: 0.3100 | Test Top-4 Acc: 0.0000\n",
      "Epoch 15 | Train Loss: 1.2325 | Train NDCG: 0.4558 | Train Top-4 Acc: 0.0413 | Val Loss: 1.6395 | Val NDCG: 0.3190 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6495 | Test NDCG: 0.3093 | Test Top-4 Acc: 0.0083\n",
      "Epoch 20 | Train Loss: 1.0701 | Train NDCG: 0.4774 | Train Top-4 Acc: 0.0504 | Val Loss: 1.6415 | Val NDCG: 0.2942 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6513 | Test NDCG: 0.3040 | Test Top-4 Acc: 0.0083\n",
      "Epoch 25 | Train Loss: 0.9815 | Train NDCG: 0.4858 | Train Top-4 Acc: 0.0513 | Val Loss: 1.6459 | Val NDCG: 0.2882 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6465 | Test NDCG: 0.3031 | Test Top-4 Acc: 0.0000\n",
      "Epoch 30 | Train Loss: 0.9704 | Train NDCG: 0.4872 | Train Top-4 Acc: 0.0552 | Val Loss: 1.6464 | Val NDCG: 0.2928 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6507 | Test NDCG: 0.2994 | Test Top-4 Acc: 0.0000\n",
      "Epoch 35 | Train Loss: 0.9675 | Train NDCG: 0.4893 | Train Top-4 Acc: 0.0545 | Val Loss: 1.6439 | Val NDCG: 0.2967 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6449 | Test NDCG: 0.3004 | Test Top-4 Acc: 0.0000\n",
      "Epoch 40 | Train Loss: 0.9679 | Train NDCG: 0.4879 | Train Top-4 Acc: 0.0504 | Val Loss: 1.6423 | Val NDCG: 0.3057 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6521 | Test NDCG: 0.3065 | Test Top-4 Acc: 0.0083\n",
      "Epoch 45 | Train Loss: 0.9660 | Train NDCG: 0.4894 | Train Top-4 Acc: 0.0580 | Val Loss: 1.6444 | Val NDCG: 0.2977 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6508 | Test NDCG: 0.3063 | Test Top-4 Acc: 0.0000\n",
      "Epoch 50 | Train Loss: 0.9668 | Train NDCG: 0.4872 | Train Top-4 Acc: 0.0547 | Val Loss: 1.6424 | Val NDCG: 0.3015 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6448 | Test NDCG: 0.3088 | Test Top-4 Acc: 0.0000\n",
      "Epoch 55 | Train Loss: 0.9650 | Train NDCG: 0.4899 | Train Top-4 Acc: 0.0564 | Val Loss: 1.6486 | Val NDCG: 0.2891 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6469 | Test NDCG: 0.3099 | Test Top-4 Acc: 0.0083\n",
      "Epoch 60 | Train Loss: 0.9658 | Train NDCG: 0.4937 | Train Top-4 Acc: 0.0652 | Val Loss: 1.6479 | Val NDCG: 0.2985 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6469 | Test NDCG: 0.3009 | Test Top-4 Acc: 0.0000\n",
      "Epoch 65 | Train Loss: 0.9673 | Train NDCG: 0.4869 | Train Top-4 Acc: 0.0520 | Val Loss: 1.6463 | Val NDCG: 0.2949 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6514 | Test NDCG: 0.3029 | Test Top-4 Acc: 0.0000\n",
      "Epoch 70 | Train Loss: 0.9668 | Train NDCG: 0.4865 | Train Top-4 Acc: 0.0516 | Val Loss: 1.6477 | Val NDCG: 0.2870 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6500 | Test NDCG: 0.3075 | Test Top-4 Acc: 0.0083\n",
      "Epoch 75 | Train Loss: 0.9667 | Train NDCG: 0.4916 | Train Top-4 Acc: 0.0586 | Val Loss: 1.6399 | Val NDCG: 0.2983 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6516 | Test NDCG: 0.3048 | Test Top-4 Acc: 0.0000\n",
      "Epoch 80 | Train Loss: 0.9661 | Train NDCG: 0.4857 | Train Top-4 Acc: 0.0551 | Val Loss: 1.6456 | Val NDCG: 0.2870 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6463 | Test NDCG: 0.3018 | Test Top-4 Acc: 0.0083\n",
      "Epoch 85 | Train Loss: 0.9673 | Train NDCG: 0.4892 | Train Top-4 Acc: 0.0555 | Val Loss: 1.6450 | Val NDCG: 0.2927 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6373 | Test NDCG: 0.3162 | Test Top-4 Acc: 0.0083\n",
      "Epoch 90 | Train Loss: 0.9653 | Train NDCG: 0.4878 | Train Top-4 Acc: 0.0553 | Val Loss: 1.6420 | Val NDCG: 0.2936 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6457 | Test NDCG: 0.3052 | Test Top-4 Acc: 0.0000\n",
      "Epoch 95 | Train Loss: 0.9676 | Train NDCG: 0.4877 | Train Top-4 Acc: 0.0559 | Val Loss: 1.6467 | Val NDCG: 0.2884 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6499 | Test NDCG: 0.3063 | Test Top-4 Acc: 0.0000\n",
      "Epoch 100 | Train Loss: 0.9665 | Train NDCG: 0.4900 | Train Top-4 Acc: 0.0613 | Val Loss: 1.6474 | Val NDCG: 0.2899 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6415 | Test NDCG: 0.3183 | Test Top-4 Acc: 0.0167\n",
      "Epoch 105 | Train Loss: 0.9684 | Train NDCG: 0.4887 | Train Top-4 Acc: 0.0561 | Val Loss: 1.6446 | Val NDCG: 0.2906 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6495 | Test NDCG: 0.3052 | Test Top-4 Acc: 0.0000\n",
      "Epoch 110 | Train Loss: 0.9664 | Train NDCG: 0.4884 | Train Top-4 Acc: 0.0592 | Val Loss: 1.6456 | Val NDCG: 0.2989 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6353 | Test NDCG: 0.3173 | Test Top-4 Acc: 0.0000\n",
      "Epoch 115 | Train Loss: 0.9657 | Train NDCG: 0.4898 | Train Top-4 Acc: 0.0626 | Val Loss: 1.6442 | Val NDCG: 0.2924 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6528 | Test NDCG: 0.3013 | Test Top-4 Acc: 0.0083\n",
      "Epoch 120 | Train Loss: 0.9658 | Train NDCG: 0.4893 | Train Top-4 Acc: 0.0592 | Val Loss: 1.6485 | Val NDCG: 0.2905 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6514 | Test NDCG: 0.2982 | Test Top-4 Acc: 0.0000\n",
      "Epoch 125 | Train Loss: 0.9675 | Train NDCG: 0.4897 | Train Top-4 Acc: 0.0558 | Val Loss: 1.6476 | Val NDCG: 0.2942 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6444 | Test NDCG: 0.3022 | Test Top-4 Acc: 0.0000\n",
      "Epoch 130 | Train Loss: 0.9685 | Train NDCG: 0.4907 | Train Top-4 Acc: 0.0622 | Val Loss: 1.6476 | Val NDCG: 0.2923 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6494 | Test NDCG: 0.3112 | Test Top-4 Acc: 0.0083\n",
      "Epoch 135 | Train Loss: 0.9647 | Train NDCG: 0.4892 | Train Top-4 Acc: 0.0508 | Val Loss: 1.6450 | Val NDCG: 0.2961 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6482 | Test NDCG: 0.2976 | Test Top-4 Acc: 0.0000\n",
      "Epoch 140 | Train Loss: 0.9671 | Train NDCG: 0.4911 | Train Top-4 Acc: 0.0633 | Val Loss: 1.6452 | Val NDCG: 0.2871 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6529 | Test NDCG: 0.3047 | Test Top-4 Acc: 0.0083\n",
      "Epoch 145 | Train Loss: 0.9659 | Train NDCG: 0.4874 | Train Top-4 Acc: 0.0562 | Val Loss: 1.6342 | Val NDCG: 0.2966 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6471 | Test NDCG: 0.3138 | Test Top-4 Acc: 0.0083\n",
      "Epoch 150 | Train Loss: 0.9690 | Train NDCG: 0.4917 | Train Top-4 Acc: 0.0528 | Val Loss: 1.6481 | Val NDCG: 0.2889 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6460 | Test NDCG: 0.3013 | Test Top-4 Acc: 0.0000\n",
      "Epoch 155 | Train Loss: 0.9659 | Train NDCG: 0.4913 | Train Top-4 Acc: 0.0575 | Val Loss: 1.6489 | Val NDCG: 0.2856 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6476 | Test NDCG: 0.3069 | Test Top-4 Acc: 0.0083\n",
      "Epoch 160 | Train Loss: 0.9666 | Train NDCG: 0.4883 | Train Top-4 Acc: 0.0571 | Val Loss: 1.6455 | Val NDCG: 0.2970 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6552 | Test NDCG: 0.2951 | Test Top-4 Acc: 0.0000\n",
      "Epoch 165 | Train Loss: 0.9656 | Train NDCG: 0.4884 | Train Top-4 Acc: 0.0565 | Val Loss: 1.6469 | Val NDCG: 0.2918 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6509 | Test NDCG: 0.3012 | Test Top-4 Acc: 0.0000\n",
      "Epoch 170 | Train Loss: 0.9654 | Train NDCG: 0.4911 | Train Top-4 Acc: 0.0608 | Val Loss: 1.6435 | Val NDCG: 0.3004 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6529 | Test NDCG: 0.3041 | Test Top-4 Acc: 0.0083\n",
      "Epoch 175 | Train Loss: 0.9658 | Train NDCG: 0.4879 | Train Top-4 Acc: 0.0556 | Val Loss: 1.6472 | Val NDCG: 0.2893 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6460 | Test NDCG: 0.3073 | Test Top-4 Acc: 0.0000\n",
      "Epoch 180 | Train Loss: 0.9696 | Train NDCG: 0.4875 | Train Top-4 Acc: 0.0557 | Val Loss: 1.6487 | Val NDCG: 0.2871 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6484 | Test NDCG: 0.3041 | Test Top-4 Acc: 0.0000\n",
      "Epoch 185 | Train Loss: 0.9681 | Train NDCG: 0.4892 | Train Top-4 Acc: 0.0553 | Val Loss: 1.6453 | Val NDCG: 0.2944 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6530 | Test NDCG: 0.2958 | Test Top-4 Acc: 0.0000\n",
      "Epoch 190 | Train Loss: 0.9670 | Train NDCG: 0.4925 | Train Top-4 Acc: 0.0622 | Val Loss: 1.6420 | Val NDCG: 0.2966 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6505 | Test NDCG: 0.2972 | Test Top-4 Acc: 0.0000\n",
      "Epoch 195 | Train Loss: 0.9661 | Train NDCG: 0.4960 | Train Top-4 Acc: 0.0685 | Val Loss: 1.6390 | Val NDCG: 0.3002 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6485 | Test NDCG: 0.3104 | Test Top-4 Acc: 0.0167\n",
      "Epoch 200 | Train Loss: 0.9675 | Train NDCG: 0.4912 | Train Top-4 Acc: 0.0602 | Val Loss: 1.6482 | Val NDCG: 0.2901 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6419 | Test NDCG: 0.3100 | Test Top-4 Acc: 0.0000\n",
      "Epoch 205 | Train Loss: 0.9619 | Train NDCG: 0.4889 | Train Top-4 Acc: 0.0577 | Val Loss: 1.6465 | Val NDCG: 0.2874 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6521 | Test NDCG: 0.2953 | Test Top-4 Acc: 0.0000\n",
      "Epoch 210 | Train Loss: 0.9679 | Train NDCG: 0.4910 | Train Top-4 Acc: 0.0586 | Val Loss: 1.6440 | Val NDCG: 0.2896 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6481 | Test NDCG: 0.3015 | Test Top-4 Acc: 0.0000\n",
      "Epoch 215 | Train Loss: 0.9664 | Train NDCG: 0.4890 | Train Top-4 Acc: 0.0602 | Val Loss: 1.6481 | Val NDCG: 0.2922 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6477 | Test NDCG: 0.3053 | Test Top-4 Acc: 0.0000\n",
      "Epoch 220 | Train Loss: 0.9648 | Train NDCG: 0.4885 | Train Top-4 Acc: 0.0592 | Val Loss: 1.6445 | Val NDCG: 0.2965 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6455 | Test NDCG: 0.3063 | Test Top-4 Acc: 0.0000\n",
      "Epoch 225 | Train Loss: 0.9687 | Train NDCG: 0.4867 | Train Top-4 Acc: 0.0535 | Val Loss: 1.6464 | Val NDCG: 0.2925 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6520 | Test NDCG: 0.3044 | Test Top-4 Acc: 0.0083\n",
      "Epoch 230 | Train Loss: 0.9671 | Train NDCG: 0.4914 | Train Top-4 Acc: 0.0602 | Val Loss: 1.6424 | Val NDCG: 0.2950 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6537 | Test NDCG: 0.3020 | Test Top-4 Acc: 0.0000\n",
      "Epoch 235 | Train Loss: 0.9681 | Train NDCG: 0.4916 | Train Top-4 Acc: 0.0634 | Val Loss: 1.6428 | Val NDCG: 0.2948 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6527 | Test NDCG: 0.3051 | Test Top-4 Acc: 0.0083\n",
      "Epoch 240 | Train Loss: 0.9660 | Train NDCG: 0.4875 | Train Top-4 Acc: 0.0551 | Val Loss: 1.6472 | Val NDCG: 0.2988 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6503 | Test NDCG: 0.3047 | Test Top-4 Acc: 0.0000\n",
      "Epoch 245 | Train Loss: 0.9655 | Train NDCG: 0.4910 | Train Top-4 Acc: 0.0584 | Val Loss: 1.6462 | Val NDCG: 0.2972 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6553 | Test NDCG: 0.3004 | Test Top-4 Acc: 0.0000\n",
      "Epoch 250 | Train Loss: 0.9664 | Train NDCG: 0.4909 | Train Top-4 Acc: 0.0613 | Val Loss: 1.6482 | Val NDCG: 0.2902 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6496 | Test NDCG: 0.3098 | Test Top-4 Acc: 0.0167\n",
      "Epoch 255 | Train Loss: 0.9668 | Train NDCG: 0.4945 | Train Top-4 Acc: 0.0672 | Val Loss: 1.6449 | Val NDCG: 0.2849 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6468 | Test NDCG: 0.3085 | Test Top-4 Acc: 0.0083\n",
      "Epoch 260 | Train Loss: 0.9667 | Train NDCG: 0.4859 | Train Top-4 Acc: 0.0506 | Val Loss: 1.6458 | Val NDCG: 0.2965 | Val Top-4 Acc: 0.0069 | Test Loss: 1.6456 | Test NDCG: 0.3084 | Test Top-4 Acc: 0.0000\n",
      "Epoch 265 | Train Loss: 0.9662 | Train NDCG: 0.4868 | Train Top-4 Acc: 0.0531 | Val Loss: 1.6444 | Val NDCG: 0.2898 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6485 | Test NDCG: 0.3068 | Test Top-4 Acc: 0.0083\n",
      "Epoch 270 | Train Loss: 0.9674 | Train NDCG: 0.4897 | Train Top-4 Acc: 0.0566 | Val Loss: 1.6505 | Val NDCG: 0.2902 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6462 | Test NDCG: 0.3037 | Test Top-4 Acc: 0.0000\n",
      "Epoch 275 | Train Loss: 0.9700 | Train NDCG: 0.4883 | Train Top-4 Acc: 0.0558 | Val Loss: 1.6468 | Val NDCG: 0.2899 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6510 | Test NDCG: 0.3181 | Test Top-4 Acc: 0.0167\n",
      "Epoch 280 | Train Loss: 0.9648 | Train NDCG: 0.4847 | Train Top-4 Acc: 0.0507 | Val Loss: 1.6457 | Val NDCG: 0.2903 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6532 | Test NDCG: 0.2986 | Test Top-4 Acc: 0.0000\n",
      "Epoch 285 | Train Loss: 0.9676 | Train NDCG: 0.4895 | Train Top-4 Acc: 0.0560 | Val Loss: 1.6485 | Val NDCG: 0.2878 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6497 | Test NDCG: 0.2992 | Test Top-4 Acc: 0.0000\n",
      "Epoch 290 | Train Loss: 0.9684 | Train NDCG: 0.4895 | Train Top-4 Acc: 0.0568 | Val Loss: 1.6491 | Val NDCG: 0.2914 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6467 | Test NDCG: 0.3072 | Test Top-4 Acc: 0.0000\n",
      "Epoch 295 | Train Loss: 0.9670 | Train NDCG: 0.4936 | Train Top-4 Acc: 0.0669 | Val Loss: 1.6466 | Val NDCG: 0.2929 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6523 | Test NDCG: 0.3038 | Test Top-4 Acc: 0.0083\n",
      "Epoch 299 | Train Loss: 0.9680 | Train NDCG: 0.4917 | Train Top-4 Acc: 0.0613 | Val Loss: 1.6493 | Val NDCG: 0.2932 | Val Top-4 Acc: 0.0000 | Test Loss: 1.6429 | Test NDCG: 0.3122 | Test Top-4 Acc: 0.0083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AttentionModel(\n",
    "    in_dim=D,\n",
    "    group_idx=idx,\n",
    "    layers=[1024, 512, 256],\n",
    "    attn_layers=5\n",
    ").to(device)\n",
    "\n",
    "_ = train(model, y, num_epochs=300, model_name=\"attention\", device=device, lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b6443",
   "metadata": {},
   "source": [
    "## Set Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934c226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
